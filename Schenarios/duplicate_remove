
dbutils.fs.put("/scenarios/duplicates.csv","""id,name,loc,updated_date
1,ravi,bangalore,2021-01-01
1,ravi,chennai,2022-02-02
1,ravi,Hyderabad,2022-06-10
2,Raj,bangalore,2021-01-01
2,Raj,chennai,2022-02-02
3,Raj,Hyderabad,2022-06-10
4,Prasad,bangalore,2021-01-01
5,Mahesh,chennai,2022-02-02
4,Prasad,Hyderabad,2022-06-10
""")
     
     
# remove duplicate from dataframe keep the latest record.......


# inferSchema gives the date columns as timestamp datatype by default.
df= spark.read.csv("/scenarios/duplicates.csv",header=True,inferSchema=True)



from pyspark.sql.functions import col
display(df.orderBy(col("updated_date").desc()).dropDuplicates(["id"]))
     
# window function with row_number()
     
from pyspark.sql.window import Window
from pyspark.sql.functions import *
df = df.withColumn("rowid",row_number().over(Window.partitionBy("id").orderBy(col("updated_date").desc())))
     

df_uniq = df.filter("rowid=1")
     
# display the bad record mean duplicate record from dataframe.
df_baddata = df.filter("rowid>1")


display(df_uniq)
     
     
